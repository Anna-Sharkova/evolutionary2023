\section{Градиентный спуск.}

  $x$ - случайная начальная точка\\ 
  \textbf{while} $x$ не минимум и время еще есть \textbf{do} \\
  $x \gets x-\alpha\nabla f(x)$\\
  \textbf{end}\\
  \textbf{return} $ x $\\

    \begin{itemize}
      \item С точки зрения оптимизации градиентный спуск представляет собой алгоритм поиска оптимума некоторого набора функций с помощью дифференциальных вычислений.
      \item Примеры: используется при обучении нейронных сетей,оптимизация нагрузок на обычные сети, оптимизация организма параллельных вычислений (оптимизация нагрузки)
      \item  Задача: минимизация функций, заданных над $R^n$. 
      \item Область применимости: функции, имеющие первые частные производные
      \item Объем вычислений за итерацию: вычисление $f$ и $\nabla f$
  \end{itemize}
   \textbf{Хорошо:} Наименьшую сложность для алгоритма представляют функции с  \textbf{одним локальным минимумом}.Если у функции выделяется оптимум по минимальному критерию применимости локальной области данной области. Т.е. без увеличения или уменьшения размерности численных значений функции при различных аргументах.
    \begin{itemize}
    \item Если частные производные разные по масштабу, работает хуже (мы за однц итерацию вычисляем не однократное значение производной, а многократное)
    \item Этот недостаток преодолевается в методе Ньютона, но нужны уже вторые частные производные. Это сужает область применимости.
    \end{itemize}
    \textbf{Плохо:} все остальные функции (если несколько локальных минимумов, нелокальные минимумы или есть минимумы с разными размерностями (когда функция задана не однозначно на всей области множества значений)
    \begin{itemize}
    \item Отсутствует гарантия нахождения глобального минимума
    \end{itemize}
    
